import pandas as pd

#Loading the Data Set 
data_1=pd.read_csv("adult.csv")
print(data_1)

data_2=data_1


#To know about the Columns

print(data_1.columns)

#To know about the Number of Rows and Columns

print(data_1.shape)

data_1.head()


#Here the values are in the format of '?' taking that consideration we can convert them into nan values

import numpy as np

data_1['workclass']=data_1['workclass'].replace('?',np.nan)

data_1['occupation']=data_1['occupation'].replace('?',np.nan)

data_1['native-country']=data_1['native-country'].replace('?',np.nan)


#To apply Algorithms with Pre-Processing some Algorithms will not work with the String (columns) we need to convert them

#Replace String values with numerical in Workclass column in Method-1

data_1['workclass']=data_1['workclass'].replace('Private',1)
data_1['workclass']=data_1['workclass'].replace('Self-emp-not-inc',2)
data_1['workclass']=data_1['workclass'].replace('Local-gov',3)
data_1['workclass']=data_1['workclass'].replace('State-gov',4)
data_1['workclass']=data_1['workclass'].replace('Self-emp-inc',5)
data_1['workclass']=data_1['workclass'].replace('Federal-gov',6)
data_1['workclass']=data_1['workclass'].replace('Without-pay',7)
data_1['workclass']=data_1['workclass'].replace('Never-worked',8)
print(data_1['workclass'].value_counts())

#Replace String values with numerical Education column in Method-1 

data_1['education']=data_1['education'].replace(to_replace='HS-grad',value=1)

data_1['education']=data_1['education'].replace(to_replace='Some-college',value=2)

data_1['education']=data_1['education'].replace(to_replace='Bachelors',value=3)

data_1['education']=data_1['education'].replace(to_replace="Masters",value=4)
data_1['education']=data_1['education'].replace(to_replace='Assoc-voc',value=5)

data_1['education']=data_1['education'].replace(to_replace='11th',value=6)

data_1['education']=data_1['education'].replace(to_replace="Assoc-acdm",value=7)

data_1['education']=data_1['education'].replace(to_replace="10th",value=8)

data_1['education']=data_1['education'].replace(to_replace="7th-8th",value=9)

data_1['education']=data_1['education'].replace(to_replace="Prof-school",value=10)
data_1['education']=data_1['education'].replace(to_replace="9th",value=11)
data_1['education']=data_1['education'].replace(to_replace="12th",value=12)
data_1['education']=data_1['education'].replace(to_replace="Doctorate",value=13)

data_1['education']=data_1['education'].replace(to_replace="5th-6th",value=14)
data_1['education']=data_1['education'].replace(to_replace="1st-4th",value=15)
data_1['education']=data_1['education'].replace(to_replace="Preschool",value=16)

#Replacing the String values with Method-2

data_1['marital-status']=data_1['marital-status'].replace({"Married-civ-spouse":1,"Never-married":2,"Divorced":3,"Separated":4,"Widowed":5,"Married-spouse-absent":6,"Married-AF-spouse":7})

#Replacing the String value for Occupation Column

data_1['occupation']=data_1['occupation'].replace({"Prof-specialty":1,"Craft-repair":2,"Exec-managerial":3,"Adm-clerical":4,"Sales":5,"Other-service":6,"Machine-op-inspct":7,"Transport-moving":8,"Handlers-cleaners":9,"Farming-fishing":10,"Tech-support":11,"Protective-serv":12,"Priv-house-serv":13,"Armed-Forces":14,"Exec-ma":15})

#Replacing the String values for Relationship column

data_1['relationship']=data_1['relationship'].replace({"Husband":1,"Not-in-family":2,"Own-child":3,"Unmarried":4,"Wife":5,"Other-relative":6})

data_1['gender']=data_1['gender'].replace({"Male":1,"Female":2})

#Replacing the String columns with values 

data_1['race']=data_1['race'].replace({"White":1,"Black":2,"Asian-Pac-Islander":3,"Amer-Indian-Eskimo":4,"Other":5})


#Replacing the Class column income with values 

data_1['income']=data_1['income'].replace({"<=50K":1,">50K":2})

print(data_1['race'].value_counts())

data_1['marital-status']=data_1['marital-status'].replace({"Married-civ-spouse":1,"Never-married":2,"Divorced":3,"Separated":4,"Widowed":5,"Married-spouse-absent":6,"Married-AF-spouse":7})


#Applying Decision Tree Algorithm
#Bagging 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn import metrics
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
from sklearn.metrics import classification_report, confusion_matrix
def func1(X_train,X_test,y_train,y_test):
               clf=DecisionTreeClassifier()
               clf=clf.fit(X_train,y_train)
               y_pre_1=clf.predict(X_test)
               print("Decision Tree Algorithm")
               print("Accuracy : ",metrics.accuracy_score(y_test,y_pre_1))
               print('Confusion Matrix : ',confusion_matrix(y_test,y_pre_1))
               print("ROC And AUC :",roc_auc_score(y_test,y_pre_1))
               print('Precision Score :',average_precision_score(y_test,y_pre_1))
               print("Bagging")
               bag_clf=BaggingClassifier(base_estimator=clf,n_estimators=1000)
               bag_clf.fit(X_train,y_train)
               y_pre_bag_1=bag_clf.predict(X_test)
               print("Accuracy :",metrics.accuracy_score(y_test,y_pre_bag_1))
               print("Confusion Matrix :",confusion_matrix(y_test,y_pre_1))
               print("ROC And AUC :",roc_auc_score(y_test,y_pre_bag_1))
               print("Precision :",average_precision_score(y_test,y_pre_bag_1))
               
               
               
  #Applying RandomForest Algorithm 

from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score
def func2(X_train,X_test,y_train,y_test):
           clf=RandomForestClassifier(n_estimators=2000)
           clf.fit(X_train,y_train)
           y_pre_2=clf.predict(X_test)
           print("RandomForest Algorithm")
           print("Accuracy : ",metrics.accuracy_score(y_test,y_pre_2))
           print("Confusion Matrix :",confusion_matrix(y_test,y_pre_2))
           print("ROC And AUC :",roc_auc_score(y_test,y_pre_2))
           print("Precision Score :",average_precision_score(y_test,y_pre_2))
  
  
  
  #Applying Naive Bayes Algorithm

from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score
def func3(X_train,X_test,y_train,y_test):
                        model=GaussianNB()
                        model.fit(X_train,y_train)
                        y_pre_3=model.predict(X_test)
                        print("Naive Bayes Algorithm")
                        print("Accuracy :",metrics.accuracy_score(y_test,y_pre_3))
                        print("Confusion Matrix ",confusion_matrix(y_test,y_pre_3))    
                        print("ROC And AUC :",roc_auc_score(y_test,y_pre_3)) 
                        print("Precision :",average_precision_score(y_test,y_pre_3))                 
  
  
  
  
  
  
  #Applying Naive Bayes Algorithm

from sklearn.naive_bayes import GaussianNB
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score
def func3(X_train,X_test,y_train,y_test):
                        model=GaussianNB()
                        model.fit(X_train,y_train)
                        y_pre_3=model.predict(X_test)
                        print("Naive Bayes Algorithm")
                        print("Accuracy :",metrics.accuracy_score(y_test,y_pre_3))
                        print("Confusion Matrix ",confusion_matrix(y_test,y_pre_3))    
                        print("ROC And AUC :",roc_auc_score(y_test,y_pre_3)) 
                        print("Precision :",average_precision_score(y_test,y_pre_3))                 




#Applying Logistic Regression 

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
def func4(X_train,X_test,y_train,y_test):
       log_reg=LogisticRegression()
       log_reg.fit(X_train,y_train)
       y_pre_4=log_reg.predict(X_test)
       print("Logisitic Regression")
       print("Accuracy : ",metrics.accuracy_score(y_test,y_pre_4))
       print("Confusion Matrix : ",confusion_matrix(y_test,y_pre_4))
       print("ROC And AUC :",roc_auc_score(y_test,y_pre_4)) 
       print("Precision :",average_precision_score(y_test,y_pre_4))




#Gradient Boosting Algorithm

from sklearn.ensemble import GradientBoostingClassifier
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import average_precision_score
from sklearn.metrics import roc_auc_score
def func5(X_train,X_test,y_train,y_test):
              gb_clf=GradientBoostingClassifier(n_estimators=1000)
              gb_clf.fit(X_train,y_train)
              y_pre_gb_1=gb_clf.predict(X_test)
              print("Gradient Boosting Algorithm")
              print("Accuracy :",metrics.accuracy_score(y_test,y_pre_gb_1))
              print("Confusion Matrix :",confusion_matrix(y_test,y_pre_gb_1))
              print("ROC And AUC :",roc_auc_score(y_test,y_pre_gb_1))
              print("Precision :",average_precision_score(y_test,y_pre_gb_1))
              


#XG Boost Algorithm 

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn import metrics
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
from sklearn.metrics import roc_auc_score

def func6(X_train,X_test,y_train,y_test):
        xgb_reg=xgb.XGBRegressor()
        xgb_reg.fit(X_train,y_train)
        y_pre_xgb_1=xgb_reg.predict(X_test)
        print("XG Boost")
        import numpy as np

        rmse=np.sqrt(mean_squared_error(y_test,y_pre_xgb_1))

        print("RMSE :%f"%(rmse))







#AdaBoost Algorithm 
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn import metrics
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.metrics import average_precision_score
def func7(X_train,X_test,y_train,y_test):
            print("Ada Boost using Random Forest")
            clf_random=RandomForestClassifier(n_estimators=20)
            clf_random.fit(X_train,y_train)
            y_clf_random=clf_random.predict(X_test)
            abc=AdaBoostClassifier(n_estimators=20,base_estimator=clf_random)
            model=abc.fit(X_train,y_train)
            y_pre=model.predict(X_test)
            print("Accuracy :",metrics.accuracy_score(y_test,y_pre))
            print("Confusion Matrix :",confusion_matrix(y_test,y_pre))
            print("ROC And AUC :",roc_auc_score(y_test,y_pre))
            print("Precision :",average_precision_score(y_test,y_pre))
 
 
 
 
 
 #Pre-Processing Technique-1

#To Know about the Missing Values 

#print(data_1.isna().sum())
import numpy as np

#They are three columns with Missing Values they are 'WorkClass','Occupation','Native-Country'


import seaborn as sns

#sns.boxplot(x=data_1['age'])


data_1['fnlwgt'].head()

data_1=pd.get_dummies(columns=['gender'],data=data_1)

import seaborn as sns

#print(sns.boxplot(data_1['capital-gain']))

#print(sns.boxplot(data_1['capital-loss']))

#Applying normaliztion for Captial-gain column and Capital-loss column

from scipy import stats

data_1['capital-gain']=np.abs(stats.zscore(data_1['capital-gain']))


data_1['capital-loss']=np.abs(stats.zscore(data_1['capital-loss']))


#Training and Testing 

from sklearn.model_selection import train_test_split


X=data_1.drop(columns=['income','native-country'])

y=data_1['income']

X_train, X_test,y_train,y_test=train_test_split(X,y,test_size=0.20)

#Filling Missing values with Imputer Missing Values Technique-1

from sklearn.impute import SimpleImputer

my_imputer=SimpleImputer()

imputer_X_train=my_imputer.fit_transform(X_train)

imputer_X_test=my_imputer.transform(X_test)

func1(imputer_X_train,imputer_X_test,y_train,y_test)

func2(imputer_X_train,imputer_X_test,y_train,y_test)

func4(imputer_X_train,imputer_X_test,y_train,y_test)

func5(imputer_X_train,imputer_X_test,y_train,y_test)

func6(imputer_X_train,imputer_X_test,y_train,y_test)

func7(imputer_X_train,imputer_X_test,y_train,y_test)




#Pre-Processing Technique-2


#To Fill Missing Values using ffill Technique-2 
data_1['workclass']=data_1['workclass'].fillna(method='ffill')

data_1['occupation']=data_1['occupation'].fillna(method='ffill')

data_1['native-country']=data_1['native-country'].fillna(method='ffill')


#Doing One-hot encoding for Race and Gender Column 

data_1=pd.get_dummies(columns=['gender','race'],data=data_1)

#Applying Normalization for for Fnlwgt beacuse they are more 20,000 outliers in the Data Set 

#Normalization is done using Z-Score Normalization

from scipy import stats

data_1['fnlwgt']=np.abs(stats.zscore(data_1['fnlwgt']))

data_1['fnlwgt'].head()

import seaborn as sns

#sns.boxplot(data_1['fnlwgt'])


#Dropping the Outliers in the fnlwgt Column after applying the Z-Score Normalization

data_1.drop(data_1[data_1['fnlwgt']>=3].index,inplace=True)

from sklearn.model_selection import train_test_split

x=data_1.drop(columns=['income','native-country'])

y=data_1['income']

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.30)

func1(X_train,X_test,y_train,y_test)

func2(X_train,X_test,y_train,y_test)

#func3(X_train,X_test,y_train,y_test)

func4(X_train,X_test,y_train,y_test)

func5(X_train,X_test,y_train,y_test)

func7(X_train,X_test,y_train,y_test)


#Pre-Processing Technique-3

#Creating an Outlier in Age column using random sample by filling it with Negative values 

import random

a=random.sample(range(0,48842),200)

for i in a:
       b=random.sample(range(-20,1),1)
       data_1.iloc[i,0]=b


#To Fill Missing Values using bfill Technique-3

data_1['workclass']=data_1['workclass'].fillna(method='bfill')

data_1['occupation']=data_1['occupation'].fillna(method='bfill')

data_1['native-country']=data_1['native-country'].fillna(method='bfill')

#Apply One-Hot Encoding for Native-Country 

data_1=pd.get_dummies(columns=['native-country'],data=data_1)

#Apply Normalization for fnlwgt, Capital-gain, Capital-loss Columns 

from scipy import stats

data_1['fnlwgt']=np.abs(stats.zscore(data_1['fnlwgt']))

data_1['capital-gain']=np.abs(stats.zscore(data_1['capital-gain']))

data_1['capital-loss']=np.abs(stats.zscore(data_1['capital-loss']))

import seaborn as sns

#sns.boxplot(data_1['fnlwgt'])
#sns.boxplot(data_1['capital-gain'])
#sns.boxplot(data_1['capital-loss'])

#Applying Log Transformation for Age Column for removing outliers

data_1['age']=data_1['age'].map(lambda i:np.log(i) if i>0 else 0)

#Applying Algorithms for the Data Set 

#Spliting the Data into Training and Testing 

from sklearn.model_selection import train_test_split

x=data_1.drop(columns='income')

y=data_1['income']

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.30)

#func3(X_train,X_test,y_train,y_test)

func4(X_train,X_test,y_train,y_test)

func5(X_train,X_test,y_train,y_test)

func7(X_train,X_test,y_train,y_test)




#Pre-Processing Technique-4

#To Fill Missing values with Interpolate Technique-4

data_1=data_1.interpolate(method="linear",limit_direction='forward')

#Here in the above interpolate it can't fill the missing values in String columns since native-country is string column it will not do it
#In that case we are dropping the natvie-country missing value 

data_1=data_1.drop(columns='native-country')

#In this DataSet Age columns is more impacting on the Class label income columns so we are converting them into the values of One and Two for better prediction 

import numpy as np

data_1['age']=np.where((data_1.age>=33.5),1,data_1.age)

data_1['age']=np.where((data_1.age!=1),2,data_1.age)

#We are applying the One Hot Encoding for the Data Set 

data_1=pd.get_dummies(columns=['age'],data=data_1)

#Here we are going to remove columns capital-loss and capital-gain columns which are not impacting on the Class label and more over they are have more outliers 

data_1=data_1.drop(columns=['capital-gain','capital-loss'])

#Applying One Hot Encoding for Marital Status 

data_1=pd.get_dummies(columns=['marital-status'],data=data_1)

#Splitting the Data into Training and Testing 

from sklearn.model_selection import train_test_split

x=data_1.drop(columns=['income'])

y=data_1['income']

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.20)

func1(X_train,X_test,y_train,y_test)

func2(X_train,X_test,y_train,y_test)

#func3(X_train,X_test,y_train,y_test)

func4(X_train,X_test,y_train,y_test)

func5(X_train,X_test,y_train,y_test)

func7(X_train,X_test,y_train,y_test)


#Pre-Processing Technique-5

#data_1=data_1.drop(columns=["native-country"])

#Applying One Hot Encoding on Education

#data_1=pd.get_dummies(columns=['education'],data=data_1)
 
#Filling the Missing values with Technique-5

import numpy as np

from sklearn.experimental import enable_iterative_imputer

from sklearn.impute import IterativeImputer

imp=IterativeImputer()

imp.fit(data_1)

data_1=imp.transform(data_1)

from sklearn.model_selection import train_test_split

data_1=pd.DataFrame(data=data_1,columns=['age', 'workclass', 'fnlwgt', 'educational-num', 'marital-status',
       'occupation', 'relationship', 'race', 'gender', 'capital-gain',
       'capital-loss', 'hours-per-week', 'native-country', 
       'education_10th', 'education_11th', 'education_12th',
       'education_1st-4th', 'education_5th-6th', 'education_7th-8th',
       'education_9th', 'education_Assoc-acdm', 'education_Assoc-voc',
       'education_Bachelors', 'education_Doctorate', 'education_HS-grad',
       'education_Masters', 'education_Preschool', 'education_Prof-school',
       'education_Some-college'])
x=data_1

y=data_2['income']

X_train,X_test,y_train,y_test=train_test_split(x,y,test_size=0.30)

imputer_X_train,imputer_X_test,y_train,y_test=(X_train,X_test,y_train,y_test)


func1(imputer_X_train,imputer_X_test,y_train,y_test)

func2(imputer_X_train,imputer_X_test,y_train,y_test)

#func3(imputer_X_train,imputer_X_test,y_train,y_test)

func4(imputer_X_train,imputer_X_test,y_train,y_test)

func5(imputer_X_train,imputer_X_test,y_train,y_test)

func6(imputer_X_train,imputer_X_test,y_train,y_test)

func7(imputer_X_train,imputer_X_test,y_train,y_test)


